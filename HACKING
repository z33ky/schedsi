The simulation is kicked off in `World.step()` (´schedsi/world.py´).
From there, the scheduler of the main `Module` (kernel) is invoked.

The execution and scheduling functions all return the time they ran.

The `Core` (`schedsi/cpu.py`) is the root for the timing information.
A `Thread` can request to run a certain time by passing the requested time to the `crunch()` function.
The `Core` will then look how much of the current time slice remains and lets time pass accordingly.
If the requested time exceeds the time slice a flag will be set indicating that an interrupt must be delivered.
The `Core` will deny any execution until the `finish_step` function is called, indicating that the kernel is handling the timer interrupt.

The `Core` is split into three parts:
	* `Core`, which itself contains mostly invariant information, like the `uid`
	* `_CoreStatus`, which contains variable information, such as the current `time_slice` and `current_time`
	* `_CoreContext`, which keeps track of the current context
The `_CoreContext` would correspond to the loaded registers and page tables.
Within the simulator this is represented by storing a reference to the currently active `Module` (process) and `Thread`.
Switching context has a cost and this is also simulated, although currently only for switching between `Module`s.

The `Core` and the `Thread` record various statistics (mostly on timing), which are currently not pulled out and evaluated.

If the call-chain returns to the `World` without the time slice being used up, the CPU will idle for the remaining slice.

All events are logged in a logger class.
These logger classes are not loggers in the sense that they expect a string and store it somewhere, instead there is a function for each relevant event and the log will aggregate the relevant information for that event.
I expect the main logger will be the binary logger (`schedsi/binarylog.py`, `class BinaryLog`), since it has a `replay()` function.
Replaying is a bit of a hack and relies on Pythons duck-typing to work via "emulation classes" that pretend to be the real deal.
If the emulation classes prove to be insufficient we might have to implement a "replay scheduler" and (probably) also store the hierarchy in the log file.
The other loggers are the `TextLog` (`schedsi/textlog.py`) and the `GraphLog` (`schedsi/graphlog.py`). Both work fine with `BinaryLog.replay()`.
The `TextLog` is the easiest to understand, since it simply pulls out some information from the `Core` it received the event from and formats it to a string.
The `BinaryLog` is less straight-forward; It aggregates 'relevant' information in a dictionary of primitive types and writes that to a MessagePack stream.
The `GraphLog` also needs to keep track of some state for drawing, for instance what the current hierarchy depth is.

The only schedulers implemented at the moment is a preemptive round robin scheduler (`schedsi/round_robin.py`) and a single-thread scheduler serving as a base class (`schedsi/scheduler.py`).

Process hierarchies are built by creating the root of the hierarchy (the kernel) and adding further modules onto it.
The `tests/simple_example.py` should give you a good idea on how it works.

Each `Module` (`schedsi/module.py`) has some threads. It has at least one, the `SchedulerThread` (`schedsi/threads.py`), which forwards execution to the scheduler.
For the kernel, the `schedule()` function is invoked from `World.step()`. For each child `Module`, the kernel should provide a `VCPUThread`, which is used to `execute()` the children's `SchedulerThread`.
There are two threads to simulate load, the bare `Thread`, which will just continuously execute until its workload is finisheed (`Thread.remaining`) and `PeriodicWorkThread`, which describes its workload in regular appearing bursts.
Furthermore, each thread can have a `start_time` at which it will begin execution.
